{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8664c2",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c558c5",
   "metadata": {},
   "source": [
    "## Read train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29337b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd().parent\n",
    "train_path = root / \"datasets\" / \"train.csv\"\n",
    "validation_path = root / \"datasets\" / \"validation.csv\"\n",
    "train = pd.read_csv(train_path)\n",
    "validation = pd.read_csv(validation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29b05f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aee96e",
   "metadata": {},
   "source": [
    "### Message length distribution\n",
    "\n",
    "Apply the 1.5xIQR rule after preprocessing. Max token number > 1.5IQR = 3353\n",
    "\n",
    "Also drop char length < 20 (lower boundary from 1.5*IQR, so a rasonable email length)\n",
    "\n",
    "The 1.5×IQR rule comes from John Tukey’s exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_lengths = train[\"Message\"].str.len()\n",
    "char_length_statistics = msg_lengths.describe()\n",
    "\n",
    "spam_msg_lengths = train[train['Spam/Ham'] == 'spam']['Message'].str.len()\n",
    "spam_char_lengths_statistics = spam_msg_lengths.describe()\n",
    "\n",
    "ham_msg_lengths = train[train['Spam/Ham'] == 'ham']['Message'].str.len()    \n",
    "ham_char_lengths_statistics =ham_msg_lengths.describe()\n",
    "\n",
    "print(char_length_statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4103c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(ham_msg_lengths, bins=50)\n",
    "plt.hist(spam_msg_lengths, bins=50)\n",
    "plt.xlabel(\"Message length (characters)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Message Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = char_length_statistics['25%']\n",
    "q3 = char_length_statistics['75%']\n",
    "\n",
    "def calculate_scaled_IQR(q1, q3, scaling_factor = 1.5):\n",
    "    IQR = q3 -q1\n",
    "    upper_boundary = int(q3 + scaling_factor*IQR)\n",
    "    lower_boundary = int(q1 - scaling_factor*IQR)\n",
    "    return upper_boundary, lower_boundary\n",
    "\n",
    "upper, lower = calculate_scaled_IQR(q1,q3)\n",
    "print(upper)\n",
    "print(lower)\n",
    "\n",
    "lower = 20 # Own decision\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff4e2a",
   "metadata": {},
   "source": [
    "### Vocabulary size estimation\n",
    "reduce to -> 30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_tokens_train_set = train['Message'].astype(str).str.split()\n",
    "vocab = set(token for msg in estimated_tokens_train_set for token in msg)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "list(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f8677",
   "metadata": {},
   "source": [
    "### Out of vocabulary rate (OOV) estimation\n",
    "high oov rate -> check for patterns that can be replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0783554",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_tokens = 0\n",
    "\n",
    "estimated_tokens_validation_set = validation['Message'].astype(str).str.split()\n",
    "\n",
    "for message in estimated_tokens_validation_set:\n",
    "    for token in message:\n",
    "        if token not in vocab:\n",
    "            oov_tokens += 1\n",
    "\n",
    "oov_rate = oov_tokens / vocab_size\n",
    "print(oov_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd4fec",
   "metadata": {},
   "source": [
    "### Patterns to replace\n",
    "\n",
    "phone numbers, and numbers are replacable\n",
    "\n",
    "uppercase words -> lowercase\n",
    "\n",
    "urls, emails are not found. UPDATE: Space is used in them....\n",
    "\n",
    "same char next to each other 3 times -> collapse them into unified 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d60b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: revise regex after normalized URLs and EMAILs (they contain spaces)\n",
    "regex_url = r'^https?://(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&//=]*)$'\n",
    "regex_email = r'\\b[a-zA-Z0-9](?:[a-zA-Z0-9._-]*[a-zA-Z0-9])?@[a-zA-Z0-9](?:[a-zA-Z0-9.-]*[a-zA-Z0-9])?\\.[a-zA-Z]{2,}\\b'\n",
    "# TODO: revise regex for phone, is it useale? Or leave only num?\n",
    "regex_phone = r'\\b(?:\\+?1[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "regex_num = r'\\b\\d+(?:\\.\\d+)?\\b'\n",
    "regex_uppercase = r'\\b[A-Za-z]*\\b'\n",
    "regex_repeated_char = r'(.)\\1{2,}'\n",
    "num_of_urls = train['Message'].str.count(regex_url).sum()\n",
    "num_of_emails = train['Message'].str.count(regex_email).sum()\n",
    "num_of_phone_numbers = train['Message'].str.count(regex_phone).sum()\n",
    "num_of_numbers = train['Message'].str.count(regex_num).sum()\n",
    "num_of_uppercase_words = train['Message'].str.count(regex_uppercase).sum()\n",
    "repeated_char_count = num_of_urls = train['Message'].str.count(regex_repeated_char).sum()\n",
    "print(num_of_urls)\n",
    "print(num_of_emails)\n",
    "print(num_of_phone_numbers)\n",
    "print(num_of_numbers)\n",
    "print(num_of_uppercase_words)\n",
    "print(repeated_char_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51124ff",
   "metadata": {},
   "source": [
    "### Most frequent spam / ham tokens\n",
    "\n",
    "stop words, and punctuations -> remove them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41416bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_raw(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "spam_tokens = []\n",
    "ham_tokens = []\n",
    "\n",
    "for _,row in train.iterrows():\n",
    "    tokens = tokenize_raw(row['Message'])\n",
    "    if row['Spam/Ham'] == 'spam':\n",
    "        spam_tokens.extend(tokens)\n",
    "    else:\n",
    "        ham_tokens.extend(tokens)\n",
    "\n",
    "spam_counter = Counter(spam_tokens).most_common(120)\n",
    "ham_counter = Counter(ham_tokens).most_common(120)\n",
    "print(spam_counter)\n",
    "print(ham_counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
