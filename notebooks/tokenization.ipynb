{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a8b070",
   "metadata": {},
   "source": [
    "TODO:  build LSTM, make notebook for LLM, build LLM OOP style???\n",
    "\n",
    "TODO: pre-tokenization EDA - message lengths, new tokens, artifact effects (max length, num_words model paramters)\n",
    "\n",
    "TODO: post-tokenization EDA - sequence lengths, vocab coverage OOV rates (model parameters)\n",
    "\n",
    "TODO: decide on the **replies** and **forwarded** messages (remove them? manual inspection notebook)\n",
    "\n",
    "TODO: 1 tokenizer for lstm one for llm, different vocab size, and different train data for the SentencePieceTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f1b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b77deaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd().parent\n",
    "train_path_preprocessed = root / \"data\" / \"corpora\" / \"processed\" / \"train-pp.csv\"\n",
    "validation_path_preprocessed = root / \"data\" / \"corpora\" / \"processed\" / \"validation-pp.csv\"\n",
    "test_path_preprocessed = root / \"data\" / \"corpora\" / \"processed\" / \"test-pp.csv\"\n",
    "train = pd.read_csv(train_path_preprocessed)\n",
    "validation = pd.read_csv(validation_path_preprocessed)\n",
    "test = pd.read_csv(test_path_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da3f2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = root / \"data\" / \"models\"\n",
    "train_tokenizer_path = models_dir / \"lstm_tokenizer_train_corpus.txt\"\n",
    "train[\"Message\"].to_csv(train_tokenizer_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5004d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model_path = models_dir / \"lstm_tokenizer.model\"\n",
    "tokenizer_model_prefix = str(tokenizer_model_path.with_suffix(''))\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=str(train_tokenizer_path),\n",
    "    model_prefix=tokenizer_model_prefix,\n",
    "    vocab_size=16000,\n",
    "    model_type='unigram',\n",
    "    character_coverage=1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ad7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[858, 824, 11, 130, 15981, 15981, 15981]\n",
      "winning prizes now!!!\n",
      "ID   858 -> '▁winning'\n",
      "ID   824 -> '▁prize'\n",
      "ID    11 -> 's'\n",
      "ID   130 -> '▁now'\n",
      "ID 15981 -> '!'\n",
      "ID 15981 -> '!'\n",
      "ID 15981 -> '!'\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(str(tokenizer_model_path))\n",
    "\n",
    "ids = sp.encode(\"winning prizes now!!!\")\n",
    "print(ids)\n",
    "text = sp.decode(ids)\n",
    "print(text)\n",
    "ids = sp.encode(text, out_type=int)\n",
    "pieces = sp.encode(text, out_type=str)\n",
    "\n",
    "for token_id, subword in zip(ids, pieces):\n",
    "    print(f\"ID {token_id:5d} -> '{subword}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43845bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
